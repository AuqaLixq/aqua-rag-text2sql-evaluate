#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„è¯„ä¼°ç³»ç»Ÿ
"""

import sys
import json
import numpy as np
from pathlib import Path

# æ·»åŠ è„šæœ¬ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "scripts"))

def test_json_serialization():
    """æµ‹è¯•JSONåºåˆ—åŒ–ä¿®å¤"""
    print("ğŸ§ª æµ‹è¯•JSONåºåˆ—åŒ–ä¿®å¤...")
    
    # æ¨¡æ‹ŸåŒ…å«numpyç±»å‹çš„æ•°æ®
    test_data = {
        'int32_value': np.int32(42),
        'float64_value': np.float64(3.14),
        'array_value': np.array([1, 2, 3]),
        'nested_dict': {
            'numpy_int': np.int32(100),
            'normal_string': 'test'
        },
        'normal_value': 'hello'
    }
    
    try:
        # å¯¼å…¥ä¿®å¤å‡½æ•°
        from run_evaluation import convert_numpy_types
        
        # è½¬æ¢æ•°æ®
        cleaned_data = convert_numpy_types(test_data)
        
        # å°è¯•JSONåºåˆ—åŒ–
        json_str = json.dumps(cleaned_data, ensure_ascii=False, indent=2)
        
        print("âœ… JSONåºåˆ—åŒ–æµ‹è¯•é€šè¿‡")
        print(f"   åŸå§‹ç±»å‹: {type(test_data['int32_value'])}")
        print(f"   è½¬æ¢åç±»å‹: {type(cleaned_data['int32_value'])}")
        return True
        
    except Exception as e:
        print(f"âŒ JSONåºåˆ—åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_debug_config():
    """æµ‹è¯•è°ƒè¯•é…ç½®"""
    print("\nğŸ§ª æµ‹è¯•è°ƒè¯•é…ç½®...")
    
    try:
        import yaml
        
        # æµ‹è¯•æ¨¡æ‹Ÿæ¨¡å¼é…ç½®
        config_path = Path(__file__).parent / "configs" / "salila_config_debug.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        mock_mode = config.get('evaluation', {}).get('mock_mode', False)
        debug_mode = config.get('evaluation', {}).get('debug_mode', False)
        
        print(f"âœ… è°ƒè¯•é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   æ¨¡æ‹Ÿæ¨¡å¼: {mock_mode}")
        print(f"   è°ƒè¯•æ¨¡å¼: {debug_mode}")
        return True
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_evaluator_init():
    """æµ‹è¯•è¯„ä¼°å™¨åˆå§‹åŒ–"""
    print("\nğŸ§ª æµ‹è¯•è¯„ä¼°å™¨åˆå§‹åŒ–...")
    
    try:
        from run_evaluation import SalilaEvaluator
        
        # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼é…ç½®
        config_path = Path(__file__).parent / "configs" / "salila_config_debug.yaml"
        evaluator = SalilaEvaluator(str(config_path))
        
        print("âœ… è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•ä¿®å¤...")
    
    tests = [
        test_json_serialization,
        test_debug_config,
        test_evaluator_init
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç°åœ¨å¯ä»¥è¿è¡Œè°ƒè¯•æ¨¡å¼äº†ã€‚")
        print("\nè¿è¡Œå‘½ä»¤:")
        print("cd Evaluation/scripts")
        print("python run_evaluation.py --config ../configs/salila_config_debug.yaml")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")

if __name__ == "__main__":
    main()
